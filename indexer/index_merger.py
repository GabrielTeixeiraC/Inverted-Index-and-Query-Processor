import heapq
import json
import os
from typing import List, Optional, Tuple, TextIO, Dict

class IndexMerger:
  """
  Class responsible for merging partial inverted indexes and document indexes
  generated by different workers into final consolidated files.
  """

  def __init__(self, index_dir: str) -> None:
    """
    Args:
      index_dir (str): Path to the directory containing partial index files.
    """
    self.index_dir = index_dir
    self.file_pointers: List[TextIO] = []
    self.heap: List[Tuple[str, List[Tuple[int, int]], TextIO]] = []

  def _read_next_token_data(self, fp: TextIO) -> Optional[Tuple[str, List[Tuple[int, int]]]]:
    """
    Reads the next token and its postings list from a JSONL file.

    Args:
      fp (TextIO): File pointer.

    Returns:
      Optional[Tuple[str, List[Tuple[int, int]]]]: (token, postings) or None if EOF.
    """
    line = fp.readline()
    if not line:
      return None
    data = json.loads(line)
    return data["token"], data["postings"]

  def merge(self):
    """
    Merges all partial inverted index files into a single final inverted index
    and generates a corresponding lexicon file with token statistics.
    """
    partial_index_files = [
      os.path.join(self.index_dir, f)
      for f in os.listdir(self.index_dir)
      if f.startswith('index_') and f.endswith('.jsonl')
    ]
    
    self.file_pointers = [open(f, 'r', encoding='utf-8') for f in partial_index_files]
    for fp in self.file_pointers:
      token_data = self._read_next_token_data(fp)
      if token_data:
        token, postings = token_data
        heapq.heappush(self.heap, (token, postings, fp))

    output_index_path = os.path.join(self.index_dir, 'final_inverted_index.jsonl')
    lexicon_path = os.path.join(self.index_dir, 'lexicon.jsonl')

    with open(output_index_path, 'w', encoding='utf-8') as index_fp, \
         open(lexicon_path, 'w', encoding='utf-8') as lexicon_fp:
      while self.heap:
        # Get the first token (alphabetically) from the heap 
        token, postings, fp = heapq.heappop(self.heap)
        merged_postings = postings

        # Merge postings for the same token
        while self.heap and self.heap[0][0] == token:
          _, more_postings, other_fp = heapq.heappop(self.heap)
          merged_postings.extend(more_postings)

          next_token_data = self._read_next_token_data(other_fp)
          if next_token_data:
            next_token, next_postings = next_token_data
            heapq.heappush(self.heap, (next_token, next_postings, other_fp))
        
        # Sort merged postings by document ID        
        merged_postings.sort(key=lambda x: x[0])

        # Write merged token to index file
        index_fp.write(json.dumps({"token": token, "postings": merged_postings}) + '\n')

        # Write token statistics to lexicon
        document_frequency = len(merged_postings)
        corpus_frequency = sum(freq for _, freq in merged_postings)

        lexicon_entry = {
          "token": token,
          "document_frequency": document_frequency,
          "corpus_frequency": corpus_frequency
        }
        
        lexicon_fp.write(json.dumps(lexicon_entry) + '\n')

        next_token_data = self._read_next_token_data(fp)
        if next_token_data:
          next_token, next_postings = next_token_data
          heapq.heappush(self.heap, (next_token, next_postings, fp))

    self._cleanup_files(self.file_pointers)

  def merge_document_indexes(self):
    """
    Merges document index files (metadata about documents) from multiple workers
    into a single sorted document index file.
    """
    document_index_files = [
      os.path.join(self.index_dir, f)
      for f in os.listdir(self.index_dir)
      if f.startswith('document_index_worker_') and f.endswith('.jsonl')
    ]

    file_pointers = []
    heap: List[Tuple[int, Dict, TextIO]] = []

    for file in document_index_files:
      fp = open(file, 'r', encoding='utf-8')
      file_pointers.append(fp)
      line = fp.readline()
      if line:
        doc = json.loads(line)
        heapq.heappush(heap, (doc['id'], doc, fp))

    output_path = os.path.join(self.index_dir, 'document_index.jsonl')

    with open(output_path, 'w', encoding='utf-8') as output_fp:
      while heap:
        # Get the smallest document ID from the heap
        _, doc, fp = heapq.heappop(heap)
        output_fp.write(json.dumps(doc) + '\n')

        line = fp.readline()
        if line:
          next_doc = json.loads(line)
          heapq.heappush(heap, (next_doc['id'], next_doc, fp))
    self._cleanup_files(file_pointers)

  def _cleanup_files(self, file_pointers: List[TextIO]) -> None:
    """
    Closes and removes the given file pointers.

    Args:
      file_pointers (List[TextIO]): List of opened file pointers to close and delete.
    """
    for fp in file_pointers:
      fp.close()
      os.remove(fp.name)
