import heapq
import json
import os
from typing import List, Optional, Tuple, TextIO, Dict

class IndexMerger:
  """
  Class responsible for merging partial inverted indexes and document indexes
  generated by different workers into final consolidated files.
  """

  def __init__(self, index_dir: str, final_index_path: str, document_index_path: str, lexicon_path: str) -> None:
    """
    Args:
      index_dir (str): Path to the directory containing partial index files.
      final_index_path (str): Path to save the final merged inverted index.
      document_index_path (str): Path to save the merged document index.
      lexicon_path (str): Path to save the lexicon file with token statistics.
    """
    self.index_dir = index_dir
    self.final_index_path = final_index_path
    self.document_index_path = document_index_path
    self.lexicon_path = lexicon_path
    self.file_pointers: List[TextIO] = []
    self.heap: List[Tuple[str, List[Tuple[int, int]], TextIO]] = []

  def _read_next_token_data(self, fp: TextIO) -> Optional[Tuple[str, List[Tuple[int, int]]]]:
    """
    Reads the next token and its postings list from a JSONL file.

    Args:
      fp (TextIO): File pointer.

    Returns:
      Optional[Tuple[str, List[Tuple[int, int]]]]: (token, postings) or None if EOF.
    """
    line = fp.readline()
    if not line:
      return None
    data = json.loads(line)
    return data["token"], data["postings"]
  
  def _save_token_to_lexicon(self, token: str, postings: List[Tuple[int, int]], lexicon_fp: TextIO) -> None:
    """
    Saves a token and its statistics to the lexicon file.
    
    Args:
      token (str): The token to save.
      postings (List[Tuple[int, int]]): The postings list for the token.
      lexicon_fp (TextIO): File pointer to the lexicon file.
    """
    # Write token statistics to lexicon
    document_frequency = len(postings)
    corpus_frequency = sum(freq for _, freq in postings)

    lexicon_entry = {
      "token": token,
      "document_frequency": document_frequency,
      "corpus_frequency": corpus_frequency
    }
    
    lexicon_fp.write(json.dumps(lexicon_entry) + '\n')

  def merge(self) -> Tuple[int, int]:
    """
    Merges all partial inverted index files into a single final inverted index
    and generates a corresponding lexicon file with token statistics.

    Returns:
      total_postings (int): Total number of postings across all tokens in the final index.
      number_of_lists (int): Total number of unique tokens in the final index.
    """
    partial_index_files = [
      os.path.join(self.index_dir, f)
      for f in os.listdir(self.index_dir)
      if f.startswith('index_') and f.endswith('.jsonl')
    ]
    
    self.file_pointers = [open(f, 'r', encoding='utf-8') for f in partial_index_files]
    for fp in self.file_pointers:
      token_data = self._read_next_token_data(fp)
      if token_data:
        token, postings = token_data
        heapq.heappush(self.heap, (token, postings, fp))

    # Initialize merge statistics
    total_postings = 0
    number_of_lists = 0

    with open(self.final_index_path, 'w', encoding='utf-8') as index_fp, \
         open(self.lexicon_path, 'w', encoding='utf-8') as lexicon_fp:
      while self.heap:
        # Get the first token (alphabetically) from the heap 
        token, postings, fp = heapq.heappop(self.heap)
        merged_postings = postings

        # Merge postings for the same token
        while self.heap and self.heap[0][0] == token:
          _, more_postings, other_fp = heapq.heappop(self.heap)
          merged_postings.extend(more_postings)

          # Read the next token from the other file pointer and push it to the heap
          next_token_data = self._read_next_token_data(other_fp)
          if next_token_data:
            next_token, next_postings = next_token_data
            heapq.heappush(self.heap, (next_token, next_postings, other_fp))
        
        # Sort merged postings by document ID        
        merged_postings.sort(key=lambda x: x[0])
        
        # Update statistics during merge
        total_postings += len(merged_postings)
        number_of_lists += 1

        # Write merged token to index file
        index_fp.write(json.dumps({"token": token, "postings": merged_postings}) + '\n')

        # Save token to lexicon
        self._save_token_to_lexicon(token, merged_postings, lexicon_fp)

        # Read the next token from the current file pointer and push it to the heap
        next_token_data = self._read_next_token_data(fp)
        if next_token_data:
          next_token, next_postings = next_token_data
          heapq.heappush(self.heap, (next_token, next_postings, fp))

    self._cleanup_files(self.file_pointers)

    return total_postings, number_of_lists

  def merge_document_indexes(self):
    """
    Merges document index files (metadata about documents) from multiple workers
    into a single sorted document index file.
    """
    document_index_files = [
      os.path.join(self.index_dir, f)
      for f in os.listdir(self.index_dir)
      if f.startswith('document_index_worker_') and f.endswith('.jsonl')
    ]

    file_pointers = []
    heap: List[Tuple[str, Dict, TextIO]] = []

    for file in document_index_files:
      fp = open(file, 'r', encoding='utf-8')
      file_pointers.append(fp)
      line = fp.readline()
      if line:
        doc = json.loads(line)
        heapq.heappush(heap, (doc['id'], doc, fp))

    with open(self.document_index_path, 'w', encoding='utf-8') as output_fp:
      while heap:
        # Get the smallest document ID from the heap
        _, doc, fp = heapq.heappop(heap)
        output_fp.write(json.dumps(doc) + '\n')

        line = fp.readline()
        if line:
          next_doc = json.loads(line)
          heapq.heappush(heap, (next_doc['id'], next_doc, fp))
    self._cleanup_files(file_pointers)

  def _cleanup_files(self, file_pointers: List[TextIO]) -> None:
    """
    Closes and removes the given file pointers.

    Args:
      file_pointers (List[TextIO]): List of opened file pointers to close and delete.
    """
    for fp in file_pointers:
      fp.close()
      os.remove(fp.name)
